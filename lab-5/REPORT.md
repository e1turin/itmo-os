# Отчет по лабораторной работе


## Задание

В соответствии с вариантом выдается набор атрибутов для утилиты нагрузочного
тестирования системы – `stress-ng`. Утилита позволяет точечно нагружать
подсистемы, что может быть полезно при тестировании конкретных компонентов
системы.

Вариант:

```
cpu:     [explog,int128]; 
cache:   [l1cache,cache-ways]; 
io:      [io-uring,ioport];
memory:  [zlib-mem-level,fork-vm]; 
network: [dccp,netlink-proc]; 
pipe:    [pipeherd,sigpipe]; 
sched:   [resched,schedpolicy]
```

## Выполнение задания

### Изучение `stress-ng`

В соответствии с вариантом были составлены скрипты использующие `stress-ng` с
соответствующими атрибутами. Для этого пришлось обратиться к документации к
утилите и понять, какие значения эти атрибуты должны принимать. 

Большинство атрибутов отвечают за количество создаемых "воркеров" (worker), 
которые выполняют определенные тестирующие сценарии, например, вычисление 
математических выражений, обмен данными через pipe-ы, создают дочерние процессы 
и вызывают перепланирование процессов, выгружают страницы виртуальной памяти и пр.

Стоит заметить, что численные параметры не строго фиксируют количество создаваемых 
`stress-ng` процессов, а именно указывают количество сущностей выполняющих тесты 
(воркеров). Уже сами эти воркеры создают нужное им количество процессов и используют
другие ресурсы.

Во всех тестовых случаях утилита нагружает систему на полную, только в случае
тестирования CPU, флаг `--cpu` ограничивал количество создаваемых подпроцессов
и если их меньше чем доступно ядер процессора, то они распределяются на них, не
нагружая все сразу.

Важно заметить, что `stress-ng` не стоит использовать как бенчмарк и брать для
рассмотрения метрики которые можно получить флагом `--metrics`, т.к. даже в
документации к утилите написано, что BOGO ops не отражают реальной
производительности системы. Для оценки нужно использовать сторонние средства для мониторинга.
Еще `strss-ng` имеет опцию `--tz`, которая согласно описанию должна выводить
показатели температуры ядер процессора в тестах, но ее использование невозможно
в виртуальной машине и, похоже, в live-режиме тоже.

### Изучение средств мониторинга системы

Для мониторнга системы использовались утилиты которые снимают показания нагруженности
в разных подсистемах. Вот пример таких:
- `sar`
- `htop`
- `vmstat`
- `pidstat`
- `perf`
- и другие.

Другие утилиты показались не очень полезными или демонстративными для
наблюдения изменения производительности системы при разных тестов.

### Снятие показаний

Для наблюдения нагрузки системы в реальном врмени было удобно использовать
терминальный мультиплексор (`tmux`) и утилиты для автоматизации (`watch`,
`timeout`) или встроенные с утилиты мониторинга возможности.

В качестве тестовой системы изначально использовалась виртаульная машина в
Ubuntu Multipass (4 ядра, 2GB RAM), но в ситу ограничения виртуальной машины на
доступ к счетчикам ядра, позже были произведены тесты системы live-kali linux
на компьютере моего соседа (12 ядерный Ryzen, 16GB RAM).

#### Мониторинг нагрузки на CPU

Атрибуты `explog` и `int128` используются для указания метода нагрузки
процессора. Они отвечают за выбор определенной вычислительные задачи для
воркеров. По большому счёту они создают одинаковую нагрузку. Поэтому решующую
роль в нагрузке процессора играет именно количество воркеров, которое
указывается параметром `--cpu`. По умолчанию, на моей тестовой системе
использовалось 32 воркера для нагрузки 12 ядер процессора.

Какие-либо графики загруженности сроить трудно, т.к. даже средняя загрузка
процессора различалась при одинаковой нагрузке. Занятно наблюдать, что
показания потребление процессора задачами в `htop` и `pidstat` отличаются, но
не сильно — похоже они используют разные подходы к вычислению уровня
потребления. Добиться значительной загрузки CPU с помощью какой-нибудь задачи
не вышло.

Единственное, что я смог выбрать в качестве показателя загруженности CPU, это
счетчики количества циклов выполнения и выполненных инструкций, которые можно
собрать через `perf -B -ecycles:u,instructions:u`. Для обоих методов показатели
сопостовимы, но для разного количества воркеров, значения растут
пропорционально до определенного момента, потом устанавливается на значениях
для теста по умолчанию (12 ядер):

| `--cpu` | cycles, млрд  | instructions, млрд |
| --- | --- | --- |
| 6  |  500 | 800 |
| 12 | 1000 | 1200 |
| 18 | 1000 | 1300 |
| 36 | 1000 | 1200 |

Таким образом, устанавливать значения больше, чем по одному воркеру на ядро, не имеет смысла.

> см. скриншоты в каталоге [logs/live-kali/cpu/](./logs/live-kali/cpu/).

Позже я решил сделать дополнительные тесты с использованием `sar` на виртуальной машине с Ubuntu:
```sh
(stress-ng --cpu 0 -t 20 &) && sar 1 20 -u
```

Тут, как и заявлялось ранее, при достижении значения параметра числа доступных CPU нагрузка перестает расти.
> см. логи `cpu-...` в каталоге [logs/vm-ubuntu/sar/](./logs/vm-ubuntu/sar/)

#### Мониторинг нагрузки на подсистему кеша

Я не смог обнаружить какой-то ярковыраженной зависисмости нагруженности кеша при тестах.
Процессор влюбом случае нагружается на 100% на всех ядрах. Утилита `vmstat` не
показала каких-то значений сильно отличающихся от обычного состояни.

> см. скриншоты в каталоге [logs/live-kali/cache/](./logs/live-kali/cache/).

Позже было принято решение сделать дополнительные тесты на Multipass Ubuntu с
помощью `sar` (`-B` и `-r`, т.к. в описании к ним встречается поняитие кеша),
но никаких закономерностей не удалось выявить.

> см. логи `l1cache-...` в каталоге [logs/vm-ubuntu/sar/](./logs/vm-ubuntu/sar/)

#### Мониторинг подсистемы ввода-вывода

При мониторинге нагрузки системы ввода-вывода впервые наблюдались какие-то
отличия от предыдущих тестов. В утилите `htop` можно было видеть, что большая
часть нагрузки ложится на ядро операционной системы. Но качественно оценить
соотношение влияние опций опять же не удалось, все ядра процессора заняты на 100%.

Опция `--io-uring` нагружает ядро ОС, но зависимость нагрузки от занчения этого
атрибута осталась неизвестной, т.к. показатели времени проведенного в `us`
(user), согласно `vmstat`, меняются непредсказуемо. А время в `sy` (system) не
меняется.

Опция `--ioport` сама по себе никак не влияет на нагрузку системы, но в
комбинации с `--io-uring` она почему-то снижает нагруженность ядра ОС, это
можно было видеть в `htop`, т.к. там теперь не целиком ядра раскрашивались в
красный цвет. Качественно оценить влияение на нагрузку так же не удалось.

Довольно занятно посмотреть на код сестов и то, как используется порт 0x80:
- https://github.com/ColinIanKing/stress-ng/blob/master/stress-ioport.c
- 4.1.3. Задержка через порты ввода/вывода в https://www.opennet.ru/docs/HOWTO-RU/mini/IO-Port-Programming.html

> см. скриншоты в каталоге [logs/live-kali/io/](./logs/live-kali/io/).

#### Мониторинг подсистемы памяти

Опции `zlib-mem-level`, `fork-vm` не создают своих воркеров. `zlib-mem-level`
указывает уровень сжатия для теста с `zlib`. 

Влияние обоих опций выяснить не удалось. Более того, их нет возможности варьировать,
т.к. `fork-vm` это просто ключ который указывает на использование `madvice` при 
создании подпроцессо, а `zlib-mem-level` может принимать значения от 1 до 9 и
по умолчанию указан как 8.

Кажется, чтобы увидеть влияние этих флагов нужно иметь более слабую систему,
для которой будет критичен уровень сжатия данных и работа с виратуальной
памятью.

> см. скриншоты в каталоге [logs/live-kali/memory/](./logs/live-kali/memory/).

#### Мониторинг cетевой подсистемы 

Опция `dccp` по какой-то причине отказалась работать на live-системе. Но как я
помню, на тестах в виртуальной машине она интересным образом нагружала систему.
Можно было в реальном времени наблюдать в `iptraf` пакеты DCCP которые
отправляются. Из зависимости можно было выявить лишь то, что чем больше
воркеров выделяется, тем больше средний трафик проходит через сетевое
устройство `lo`.

| `--dccp`   | avg traf, Мбит/с |
| --- | --- |
| 1 | 373 |
| 4 | 574 |
| 8 | 890 |
| 16| 978 |

> см. скриншоты в каталоге [logs/vm-ubuntu/network/](./logs/vm-ubuntu/network/).

Влияние опции `netlink-proc` на нагрузку системы не удалось. Она создает
воркеров, которые спавнят дочерние процессы и общаются с родителями через
`netlink` подключение. Для мониторинга использовалась утилита `nload`, которая
не показала зависимости изменения трафика от разныx значений этой опции.

> см. скриншоты в каталоге [logs/live-kali/network/](./logs/live-kali/network/).

#### Мониторинг подсистемы pipe

При мониторинге опции `pipeherd` возникли трудности в отслеживании загрузки системы,
т.к. при запуске тестов терминал зависал и показатели утилит не обновлялись
достаточно часто. 

Опция `sigpipe` никак не отражалась на общей загруженности системы и
зависимости от нее никакой не удалось получить.

В обоих случаях в утилите `vmstat` можно было наблюдать достаточно большое
количество context switch'ей в стравнении с остальными тестами.

> см. скриншоты в каталоге [logs/live-kali/pipe/](./logs/live-kali/pipe/).

Позже было принято решение сделать дополнительные тесты на Multipass Ubuntu с
помощью `sar -r`. Как можно заметить, отличается `kbcommit` и `%commmit`,
которые обозначают количество памяти требуемое для текущей работы (в том числе
swap) 

> см. логи `pipeherd-...-r.log` в каталоге [logs/vm-ubuntu/sar/](./logs/vm-ubuntu/sar/)

| `--pipeherd` | `kbcommit` | `%commit` |
| --- | --- | --- |
| 1 | 1614824     | 83.09  |
| 2 | 2738548     | 140.91 |
| 3 | 3862100     | 198.72 |
| 4 | 4985664     | 256.54 |
| 8 | 9290428     | 478.04 |

Тесты с `sar -B` не выявили никаких отличий.

#### Мониторинг подсистемы планировщика

При тестировании планировщика в `htop` можно было наблюдать появление нагрузки
процессов с низким приоритетом (синий цвет). И как можно было ожидать, довольно
большое в сравнении с другими тестами время context switch'ев которое можно
наблюдать в `vmstat`.

Влияние значений опций `resched` и `schedpolicy` отследить не удалось.
Измерение количества прерываний с помощью `sar` тоже не дали никаких успехов. 

> см. скриншоты в каталоге [logs/live-kali/sched/](./logs/live-kali/sched/).

### Выводы и результаты

В ходе выполнения работы, я изучил процесс мониторинга систем с использованием
различных утилит. Так же попробовал нагружать систему с помощью утилиты `stress-ng`.

Так же я выяснил, что отследить как именно нагружает `stress-ng` и как влияют
опции на ее работу глядя со стороны (через средства мониторинга) не так легко.
Думаю, что эту утилиту нужно использовать для создания дополнительной нагрузки
к уже рабочим программам, чтобы проверять именно их работоспособность, потому
что иначе трудно контролировать уровень нагрузки, который эта утилита создаст
на систему.

Возможно, проблемы с выявлением зависимостей от выданных мне по заданию парметров
вызваны неправильным использованием утилит. Или на сильнопроизводительной системе
эти параметры не сказываются должным образом.

В конечном счете, можно сказать, что наибольшую нагрузку `stress-ng` создает
при большем количестве созданных воркеров и большем количестве флагов
нагружающих разные подсистемы. 


